services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks: [sandbox]
    volumes:
      - ./persist/ollama:/root/.ollama
    ports:
      - "11434:11434" # optional; remove if you only want internal access

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    platform: linux/amd64
    entrypoint: ["litellm"]
    command: ["--config", "/config/config.yaml", "--port", "4000"]
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./persist/litellm:/config
    networks: [sandbox]
    ports:
      - "4000:4000"
    depends_on:
        - ollama

  openhands:
    image: ghcr.io/all-hands-ai/openhands:0.28
    container_name: openhands
    networks: [sandbox]
    depends_on:
      - litellm
    ports:
      - "3000:3000"
    environment:
      # OpenHands is typically configured via env vars for runtime + LLM endpoint
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=ghcr.io/all-hands-ai/runtime:0.28-nikolaik
      - LOG_ALL_EVENTS=true

      # Point OpenHands at LiteLLM (OpenAI-compatible /v1 endpoint)
      - LLM_API_KEY=litellm
      - LLM_MODEL=litellm_proxy/qwen2_5-coder-7b
      - LLM_BASE_URL=http://litellm:4000/v1

      # Bind mount path used by OpenHands (matches your sandboxed workspace)
      - WORKSPACE_MOUNT_PATH=/opt/workspace_base
      - SANDBOX_USER_ID=${C_UID}
    volumes:
      # Persist OpenHands state under persist/
      - ./persist/openhands-state:/.openhands-state

      # OpenHands uses Docker as its sandbox runtime (needs the socket)
      - /var/run/docker.sock:/var/run/docker.sock

      # Only mount your controlled workspace (no full host FS)
      - ./persist/workspace:/opt/workspace_base

    extra_hosts:
      - "host.docker.internal:host-gateway"
    stdin_open: true
    tty: true

  openhands-cli:
    build:
      context: ./openhands-cli
      dockerfile: Dockerfile
    container_name: openhands-cli
    working_dir: /workspace
    networks: [sandbox]
    depends_on:
      - litellm
    environment:
      - HOME=/persist/openhands-cli

      # LiteLLM OpenAI-compatible endpoint (internal Docker network)
      - OPENAI_API_BASE=http://litellm:4000/v1
      - OPENAI_API_KEY=litellm

      # Pick a model id that LiteLLM exposes (recommended: openai/â€¦ alias)
      - OPENAI_MODEL=litellm_proxy/qwen2_5-coder-7b
    volumes:
      - ./persist/workspace:/workspace
      - ./persist/openhands-cli:/persist/openhands-cli
    tty: true
    stdin_open: true

  vim:
    build:
      context: ./vim-sandbox
      args:
        C_UID: ${C_UID}
        C_GID: ${C_GID}
        USERNAME: ${USERNAME}
    container_name: vim-sandbox
    depends_on:
      - ollama
    tty: true
    stdin_open: true
    environment:
      - USERNAME=${USERNAME}
      - OLLAMA_BASE_URL=http://ollama:11434
      - WORKDIR=/workspace
    # Only mount explicitly allowed directories; no full-host filesystem exposure
    volumes:
      - ./persist/home:/home/${USERNAME}
      - ./persist/workspace:/workspace
      - ./persist/vim:/home/${USERNAME}/.vim
    working_dir: /workspace

networks:
  sandbox:
    driver: bridge
